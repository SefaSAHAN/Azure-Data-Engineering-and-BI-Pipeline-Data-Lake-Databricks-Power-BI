{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf61212-637d-479d-8370-0250faa604d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "sales_order_detail_path = \"/mnt/businesscase/silver/SalesOrderDetailParquet\"\n",
    "product_path = \"/mnt/businesscase/silver/ProductParquet\"\n",
    "\n",
    "# Read the Parquet  table\n",
    "product_df = spark.read.parquet(product_path)\n",
    "sales_order_detail_df=spark.read.parquet(sales_order_detail_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77efe5e9-27d3-4507-aa0d-8537bdbc8067",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Perform the join using 'ProductID' as the common key\n",
    "joined_df = sales_order_detail_df.join(product_df, 'ProductID', 'inner') \\\n",
    "    .withColumn('TotalListPrice', col('OrderQty') * col('ListPrice')) \\\n",
    "    .withColumn('TotalCost', col('OrderQty') * col('StandardCost'))\n",
    "\n",
    "selected_columns = sales_order_detail_df.columns + ['TotalListPrice', 'TotalCost']\n",
    "sales_order_detail_df = joined_df.select(selected_columns)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "febd138d-36f3-4b9e-9a50-d22e4b5d009c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Calculate and add the 'TotalDiscount' column\n",
    "sales_order_detail_df = sales_order_detail_df.withColumn('TotalDiscount', (col('TotalListPrice') * col('UnitPriceDiscount')))\n",
    "\n",
    "# Calculate and add the 'NewLineTotal' column\n",
    "sales_order_detail_df = sales_order_detail_df.withColumn('NewLineTotal', (col('TotalListPrice') - col('TotalDiscount')))\n",
    "\n",
    "# Calculate and add the 'TotalProfit' column\n",
    "sales_order_detail_df = sales_order_detail_df.withColumn('TotalProfit', (col('NewLineTotal') - col('TotalCost')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47614ef-bf74-46c0-a468-68471e11a031",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select only the new columns you want to display\n",
    "#selected_new_columns = ['TotalListPrice', 'TotalCost', 'TotalDiscount', 'NewLineTotal', 'TotalProfit']\n",
    "#new_columns_df = sales_order_detail_df.select(selected_new_columns)\n",
    "\n",
    "# Display the DataFrame with the selected new columns using the 'display' function\n",
    "#display(new_columns_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b91b2d-32ab-4627-ac0a-b9dda9eae00d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame as a Parquet file in the \"gold\" folder\n",
    "path=\"/mnt/businesscase/gold/SalesOrderDetailParquet\"\n",
    "sales_order_detail_df = sales_order_detail_df.coalesce(1)\n",
    "sales_order_detail_df = sales_order_detail_df.repartition(1)\n",
    "sales_order_detail_df.coalesce(1).write.parquet(path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2485fff7-df28-440d-ab9e-49f54d73e92b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Move data to silver folder as parquet format\n",
    "\n",
    "parquet_list=[\"CustomerParquet\",\"AddressParquet\",\"ProductParquet\"]\n",
    "\n",
    "for address in parquet_list:\n",
    "    path = \"/mnt/businesscase/silver/\"\n",
    "    # Read the Delta files into a DataFrame\n",
    "    path+=address\n",
    "    # Read the Delta table\n",
    "    product_df = spark.read.parquet(path)\n",
    "    path=path.replace('silver','gold')\n",
    "    \n",
    "    # Save the DataFrame as a Parquet file in the \"gold\" folder\n",
    "    product_df.coalesce(1).write.parquet(path, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3SilverToGold",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
