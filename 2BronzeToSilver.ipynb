{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1183bb0d-2f47-41a6-ac78-96c70989b028",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_path=\"/mnt/businesscase/bronze/CustomerParquet\"\n",
    "sales_order_header_path = \"/mnt/businesscase/bronze/SalesOrderHeaderParquet\"\n",
    "sales_order_detail_path = \"/mnt/businesscase/bronze/SalesOrderDetailParquet\"\n",
    "\n",
    "\n",
    "customer_df = spark.read.parquet(customer_path)\n",
    "sales_order_header_df = spark.read.parquet(sales_order_header_path)\n",
    "sales_order_detail_df = spark.read.parquet(sales_order_detail_path)\n",
    "\n",
    "# Perform a join operation to add 'CustomerID' to sales_order_detail_df\n",
    "sales_order_detail_df = sales_order_detail_df.join(sales_order_header_df.select('SalesOrderID', 'CustomerID'), 'SalesOrderID', 'inner')\n",
    "\n",
    "\n",
    "# Select only 'SalesOrderID' and 'ShipToAddressID' columns from sales_order_header_df\n",
    "ship_to_address_df = sales_order_header_df.select(\"SalesOrderID\", \"ShipToAddressID\")\n",
    "\n",
    "# Join sales_order_detail_df and ship_to_address_df on 'SalesOrderID'\n",
    "# This helps add the 'ShipToAddressID' column to sales_order_detail_df\n",
    "sales_order_detail_df = sales_order_detail_df.join(ship_to_address_df, \"SalesOrderID\", \"left\")\n",
    "\n",
    "#display(sales_order_detail_with_ship_to_address)\n",
    "# Show the resulting DataFrame with the added 'CustomerID' column\n",
    "#display(sales_order_detail_df)\n",
    "#sales_order_detail_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "999d9b65-2589-46dc-8387-1904be688502",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "product_path = \"/mnt/businesscase/bronze/ProductParquet\"\n",
    "\n",
    "# Read the Parquet  table\n",
    "product_df = spark.read.parquet(product_path)\n",
    "\n",
    "\n",
    "# Cast 'StandardCost' and 'ListPrice' to DoubleType and format them correctly\n",
    "product_df = product_df.withColumn(\"StandardCost\", regexp_replace(col(\"StandardCost\"), \",\", \".\").cast(DoubleType()))\n",
    "product_df = product_df.withColumn(\"ListPrice\", regexp_replace(col(\"ListPrice\"), \",\", \".\").cast(DoubleType()))\n",
    "\n",
    "# Assuming 'UnitPriceDiscount' contains values like '0,40'\n",
    "sales_order_detail_df = sales_order_detail_df.withColumn(\"UnitPriceDiscount\", regexp_replace(col(\"UnitPriceDiscount\"), \",\", \".\").cast(DoubleType()))\n",
    "\n",
    "# Save the DataFrame as a Parquet file in the \"silver\" folder\n",
    "sales_order_detail_path = \"/mnt/businesscase/silver/SalesOrderDetailParquet\"\n",
    "sales_order_detail_df.coalesce(1).write.parquet(sales_order_detail_path, mode=\"overwrite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce385220-f78b-4a53-8d04-1441a96d575b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "address_path = \"/mnt/businesscase/bronze/AddressParquet\"\n",
    "customer_address_path = \"/mnt/businesscase/bronze/CustomerAddressParquet\"\n",
    "\n",
    "address_df = spark.read.parquet(address_path)\n",
    "customer_address_df = spark.read.parquet(customer_address_path)\n",
    "\n",
    "# Check for duplicate values using the 'AddressID' column\n",
    "duplicate_address_ids = customer_address_df.groupBy(\"AddressID\").count().filter(col(\"count\") > 1)\n",
    "\n",
    "# Show the duplicate 'AddressID' values\n",
    "#duplicate_address_ids.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918a21c8-0358-4cd9-90e4-58027f60b03c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join address_df with customer_address_df on 'AddressID'\n",
    "# This helps add the 'CustomerID' column to address_df\n",
    "address_with_customer_id = address_df.join(customer_address_df, \"AddressID\", \"left\")\n",
    "\n",
    "# Select only the 'AddressID' and 'CustomerID' columns\n",
    "address_df = address_with_customer_id.select(\"AddressID\", \"CustomerID\",\"City\",\"StateProvince\",\"CountryRegion\")\n",
    "\n",
    "product_path = \"/mnt/businesscase/silver/AddressParquet\"\n",
    "address_df.coalesce(1).write.parquet(product_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62e27fb-3b0e-4277-a00c-5cc00bfcde87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "productcategory_path = \"/mnt/businesscase/bronze/ProductCategoryParquet\"\n",
    "productsubcategory_path = \"/mnt/businesscase/bronze/ProductSubCategoryParquet\"\n",
    "\n",
    "productcategory_df = spark.read.parquet(productcategory_path)\n",
    "productsubcategory_df = spark.read.parquet(productsubcategory_path)\n",
    "\n",
    "productcategory_df = productcategory_df.withColumnRenamed(\"ProductCategoryID\", \"ParentProductCategoryID\")\n",
    "productcategory_df = productcategory_df.withColumnRenamed(\"Name\", \"ParentCategoryName\")\n",
    "\n",
    "productcategory_df = productsubcategory_df.join(productcategory_df, \"ParentProductCategoryID\", \"left\")\n",
    "\n",
    "product_df = product_df.join(productcategory_df, \"ProductCategoryID\", \"left\")\n",
    "\n",
    "product_path = \"/mnt/businesscase/silver/ProductParquet\"\n",
    "product_df.coalesce(1).write.parquet(product_path, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3dfcf6a-aa27-4a0c-a35c-abc684fabd44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_path = \"/mnt/businesscase/silver/CustomerParquet\"\n",
    "customer_df.coalesce(1).write.parquet(product_path, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2BronzeToSilver",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
